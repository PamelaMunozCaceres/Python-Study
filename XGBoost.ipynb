{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que hace tan popular a XGBoost es la velocidad y los resultados que logra alcanzar. El algoritmos es paralelizable y por ende logra ser rápido de entrenar, se puede paralelizar en la GPU y entre una red de computadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost ha logrado una performance al nivel del estado del arte en muchas tareas de ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Clasificación con XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos el dataset de cancer de mamas (de ML con árboles de decisión) para crear un modelo de XGBoost rápido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "# Traemos los datos\n",
    "wbc = pd.read_csv(\"datasets/MLTreeModels/wbc.csv\")\n",
    "# Reemplazamos por 1 y 0\n",
    "wbc['diagnosis'] = wbc['diagnosis'].replace(['M', 'B'], [1, 0])\n",
    "X = wbc[['radius_mean', 'texture_mean', 'perimeter_mean',\n",
    "         'area_mean', 'smoothness_mean', 'compactness_mean',\n",
    "         'concavity_mean', 'concave points_mean', 'symmetry_mean',\n",
    "         'fractal_dimension_mean', 'radius_se', 'texture_se',\n",
    "         'perimeter_se', 'area_se', 'smoothness_se',\n",
    "         'compactness_se', 'concavity_se', 'concave points_se',\n",
    "         'symmetry_se', 'fractal_dimension_se', 'radius_worst',\n",
    "         'texture_worst', 'perimeter_worst', 'area_worst',\n",
    "         'smoothness_worst', 'compactness_worst', 'concavity_worst',\n",
    "         'concave points_worst', 'symmetry_worst',\n",
    "         'fractal_dimension_worst']]\n",
    "y = wbc[[\"diagnosis\"]]\n",
    "\n",
    "# Dividimos los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                   random_state=123)\n",
    "\n",
    "# Instanciamos el objeto\n",
    "xg_cl = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                          n_estimators=10, seed=123)\n",
    "\n",
    "# Ajustamos a los datos de entrenamiento\n",
    "xg_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predecimos\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "# Evaluamos\n",
    "score = accuracy_score(y_test, preds)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost es un ensamble algorithm ya que usa el resultado de muchos otros modelos para poder predecir. Usa los árboles de decisión como base learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting se puede entender como un concepto que se aplica a un set de modelos de machine learning, es un algoritmo de ensamble. Se usa para convertir un conjunto de weak learners en un strong learner (cualquier algoritmo que pueda ser perfeccionado para lograr una mejor performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation en XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usar cross-validation directamente con xgboost debemos transformar la data en un formato especifico llamado DMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9525\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Traemos los datos\n",
    "wbc = pd.read_csv(\"datasets/MLTreeModels/wbc.csv\")\n",
    "# Reemplazamos por 1 y 0\n",
    "wbc['diagnosis'] = wbc['diagnosis'].replace(['M', 'B'], [1, 0])\n",
    "X = wbc[['radius_mean', 'texture_mean', 'perimeter_mean',\n",
    "         'area_mean', 'smoothness_mean', 'compactness_mean',\n",
    "         'concavity_mean', 'concave points_mean', 'symmetry_mean',\n",
    "         'fractal_dimension_mean', 'radius_se', 'texture_se',\n",
    "         'perimeter_se', 'area_se', 'smoothness_se',\n",
    "         'compactness_se', 'concavity_se', 'concave points_se',\n",
    "         'symmetry_se', 'fractal_dimension_se', 'radius_worst',\n",
    "         'texture_worst', 'perimeter_worst', 'area_worst',\n",
    "         'smoothness_worst', 'compactness_worst', 'concavity_worst',\n",
    "         'concave points_worst', 'symmetry_worst',\n",
    "         'fractal_dimension_worst']]\n",
    "y = wbc[[\"diagnosis\"]]\n",
    "churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "params = {\"objective\": \"binary:logistic\", \"max_depth\":4}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=4,\n",
    "                    num_boost_round=10, metrics=\"error\", as_pandas=True)\n",
    "\n",
    "performance = round((1 - cv_results[\"test-error-mean\"]).iloc[-1], 4)\n",
    "print(f\"Accuracy: {performance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost es útil cuando se tienen grandes cantidades de datos (aunque basta con el numero de caracteristicas sea menor al numero de filas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciona bien cuando hay una mezcla de variables categoricas y numericas, o cuando son solo numericas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No es un buen algoritomo cuandos se trata de reconocimiento de imagenes, computer vision o NLP (para lo que es mejor usar Deep Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Regresión con XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predecir valores continuos, en la mayoría de los casos se usa RMSE como métrica para medire la performanca, aunque tambien se usa frecuentemente MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Funciones objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miden que tan lejos estan las predicciones de los valores reales, el objetivo es encontrar el modelo que minimice el valor de la funcion objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En XGBoost las funciones objetivo mas comunes son reg:linear (para problemas de regresión), reg:logistic (cuando se busca la clasificación de un registro) y binary:logistic (cuando se quiere saber la probabilidad de un suceso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Base Learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost combina muchos modelos individuales para poder predecir, cada uno de esos modelos individuales se conoce como base learner. Lo que se busca es que estos base learner sean un poco mejor que un modelo aleatorio en predecir una parte especifica del dataset y uniformamente malos en predecir lo demás"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crime</th>\n",
       "      <th>zone</th>\n",
       "      <th>industry</th>\n",
       "      <th>charles</th>\n",
       "      <th>no</th>\n",
       "      <th>rooms</th>\n",
       "      <th>age</th>\n",
       "      <th>distance</th>\n",
       "      <th>radial</th>\n",
       "      <th>tax</th>\n",
       "      <th>pupil</th>\n",
       "      <th>aam</th>\n",
       "      <th>lower</th>\n",
       "      <th>med_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     crime  zone  industry  charles     no  rooms   age  distance  radial  \\\n",
       "0  0.00632  18.0      2.31        0  0.538  6.575  65.2    4.0900       1   \n",
       "1  0.02731   0.0      7.07        0  0.469  6.421  78.9    4.9671       2   \n",
       "2  0.02729   0.0      7.07        0  0.469  7.185  61.1    4.9671       2   \n",
       "3  0.03237   0.0      2.18        0  0.458  6.998  45.8    6.0622       3   \n",
       "4  0.06905   0.0      2.18        0  0.458  7.147  54.2    6.0622       3   \n",
       "\n",
       "     tax  pupil     aam  lower  med_price  \n",
       "0  296.0   15.3  396.90   4.98       24.0  \n",
       "1  242.0   17.8  396.90   9.14       21.6  \n",
       "2  242.0   17.8  392.83   4.03       34.7  \n",
       "3  222.0   18.7  394.63   2.94       33.4  \n",
       "4  222.0   18.7  396.90   5.33       36.2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "names =  [\"crime\", \"zone\", \"industry\", \"charles\", \"no\", \"rooms\", \"age\", \"distance\", \"radial\",\n",
    "          \"tax\", \"pupil\", \"aam\", \"lower\", \"med_price\"]\n",
    "boston_data = pd.read_csv(\"datasets/XGBoost/housing.csv\", names=names, sep='\\s+')\n",
    "boston_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = boston_data.iloc[:, :-1], boston_data.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=123)\n",
    "\n",
    "xg_reg = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=10,\n",
    "                          seed=123)\n",
    "\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "preds = xg_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3.7824432076699046\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función objetivo no solo nos indica que tan bueno es el modelo, sino que tambien que tan complejo es. Se le dice regularizacion a la idea de penalizar los modelos mientras estos se van volviendo complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea es obtener un modelo preciso, pero a la vez lo más simple posible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos de los parametros que permite aplicar regularización en XGBoost son:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma: Controla si un nodo se va a dividir o no basado en la reducción esperada de la función objetivo luego de hacer la división.  Mientras mayor es gamma, menos divisiones debiesen hacerse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha: L1 Regularization, penaliza el peso de las hojas. Mientras mayor el valor, mayor sera la regularizacion y el peso de las hojas tenderá a cero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda: L2 Regularization. Es una penalización un poco más sueve que la de L1. LLevan a los pesos de las hojas a descender suavemente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best rmse as a function of l1:\n",
      "    l1      rmse\n",
      "0    1  3.461474\n",
      "1   10  3.821152\n",
      "2  100  4.645518\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "names =  [\"crime\", \"zone\", \"industry\", \"charles\", \"no\", \"rooms\", \"age\", \"distance\", \"radial\",\n",
    "          \"tax\", \"pupil\", \"aam\", \"lower\", \"med_price\"]\n",
    "boston_data = pd.read_csv(\"datasets/XGBoost/housing.csv\", names=names, sep='\\s+')\n",
    "X, y = boston_data.iloc[:, :-1], boston_data.iloc[:,-1]\n",
    "boston_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "params = {\"objetive\": \"reg:linear\", \"max_depth\":4, \"verbosity\":0}\n",
    "\n",
    "l1_params = [1, 10, 100]\n",
    "rmses_l1 = []\n",
    "\n",
    "for reg in l1_params:\n",
    "    params[\"alpha\"] = reg\n",
    "    cv_results = xgb.cv(dtrain=boston_dmatrix, params=params, nfold=4,\n",
    "                        num_boost_round=10, metrics=\"rmse\", as_pandas=True,\n",
    "                        seed=123)\n",
    "    rmses_l1.append(cv_results[\"test-rmse-mean\"].tail(1).values[0])\n",
    "    \n",
    "print(\"Best rmse as a function of l1:\")\n",
    "print(pd.DataFrame(list(zip(l1_params, rmses_l1)), columns=[\"l1\", \"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Fine-Tunining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos como sería un entrenamiento sin optimizar los parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El RMSE del test es 3.43206225\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "names =  [\"crime\", \"zone\", \"industry\", \"charles\", \"no\", \"rooms\", \"age\", \"distance\", \"radial\",\n",
    "          \"tax\", \"pupil\", \"aam\", \"lower\", \"med_price\"]\n",
    "boston_data = pd.read_csv(\"datasets/XGBoost/housing.csv\", names=names, sep='\\s+')\n",
    "X, y = boston_data.iloc[:, :-1], boston_data.iloc[:,-1]\n",
    "boston_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "params = {\"objetive\": \"reg:linear\", \"verbosity\":0}\n",
    "\n",
    "# cross validation\n",
    "cv_results = xgb.cv(dtrain=boston_dmatrix, params=params, nfold=4,\n",
    "                    metrics=\"rmse\", as_pandas=True,\n",
    "                    seed=123)\n",
    "\n",
    "# Evaluacion\n",
    "rmse_test = cv_results[\"test-rmse-mean\"].tail(1).values[0]\n",
    "print(f\"El RMSE del test es {rmse_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora probamos con un modelo modificando algunos parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El RMSE del test es 3.4437465\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "names =  [\"crime\", \"zone\", \"industry\", \"charles\", \"no\", \"rooms\", \"age\", \"distance\", \"radial\",\n",
    "          \"tax\", \"pupil\", \"aam\", \"lower\", \"med_price\"]\n",
    "boston_data = pd.read_csv(\"datasets/XGBoost/housing.csv\", names=names, sep='\\s+')\n",
    "X, y = boston_data.iloc[:, :-1], boston_data.iloc[:,-1]\n",
    "boston_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "params = {\"objective\": \"reg:linear\", \"verbosity\":0, \"colsample_bytree\": 0.3,\n",
    "          \"learning_rate\": 0.03, \"max_depth\": 4}\n",
    "\n",
    "# cross validation\n",
    "cv_results = xgb.cv(dtrain=boston_dmatrix, params=params, nfold=4,\n",
    "                    metrics=\"rmse\", as_pandas=True, num_boost_round=200,\n",
    "                    seed=123)\n",
    "\n",
    "# Evaluacion\n",
    "rmse_test = cv_results[\"test-rmse-mean\"].tail(1).values[0]\n",
    "print(f\"El RMSE del test es {rmse_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteramos sobre una lista de posibilidades para encontrar el mejor rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_boost_round       rmse\n",
      "0                5  20.851624\n",
      "1               10  18.234990\n",
      "2               15  16.062435\n",
      "3               17  15.281037\n",
      "4               19  14.535187\n",
      "5               20  14.183934\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "names =  [\"crime\", \"zone\", \"industry\", \"charles\", \"no\", \"rooms\", \"age\", \"distance\", \"radial\",\n",
    "          \"tax\", \"pupil\", \"aam\", \"lower\", \"med_price\"]\n",
    "boston_data = pd.read_csv(\"datasets/XGBoost/housing.csv\", names=names, sep='\\s+')\n",
    "X, y = boston_data.iloc[:, :-1], boston_data.iloc[:,-1]\n",
    "boston_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "params = {\"objective\": \"reg:linear\", \"verbosity\":0, \"colsample_bytree\": 0.3,\n",
    "          \"learning_rate\": 0.03, \"max_depth\": 4}\n",
    "\n",
    "# Definimos distintas posibilidades de num_boost_round\n",
    "num_rounds = [5, 10, 15, 17, 19, 20]\n",
    "\n",
    "# Iteramos por cada posible combinacion\n",
    "final_rmse_per_round = []\n",
    "for n_rounds in num_rounds:\n",
    "    cv_results = xgb.cv(dtrain=boston_dmatrix, params=params, nfold=3,\n",
    "                        num_boost_round=n_rounds, metrics=\"rmse\",\n",
    "                        as_pandas=True, seed=123)\n",
    "    # Agregamos a la lista de resultados\n",
    "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "    \n",
    "# Unimos los resultados\n",
    "num_rounds_rmses = pd.DataFrame(list(zip(num_rounds, final_rmse_per_round)),\n",
    "                                columns=[\"num_boost_round\", \"rmse\"])\n",
    "print(num_rounds_rmses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Tunable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para modelos basados en arboles se pueden ajustar los siguientes parametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - learning rate: Que tan rápido se ajusta el modelo al error residual\n",
    "    - gamma: Cual es el minimo de ganacia en la funcion objetivo para que un nodo se divida\n",
    "    - lambda: Regularización L2\n",
    "    - alpha: Regularización L1\n",
    "    - max_depth: que tanto puede crecer un arbol en las rondas de boosting\n",
    "    - subsample: (va entre 0 y 1) El porcentaje de datos que se utilizará para entrenar el modelo en cada ronda de Boosting\n",
    "    - colsample_bytree: Fracción de caracteristicas que se pueden usar en cada ronda de boosting (es un valor entre 0 y 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para modelos basados en regresiones lineales se puede modificar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - lambda: Regularización L2\n",
    "    - alpha: Regularización L1\n",
    "    - lambda_bias: L2 en bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ambos es posible modificar el numero de estimadores que se usan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 GridSearch y RandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como escogemos la mejor combinación de parametros cuando estos interactuan en relaciones no lineales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search es un método que busca exhaustivamente sobre un set de hiperparametros, esto quiere decir, que el numero de modelos calculados será igual a la suma de los productos de el numero de hiperparametros a optimizar por la cantidad de opciones en cada uno de estos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parametros {'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Mejor score -20.346251013052715\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load data\n",
    "names =  [\"crime\", \"zone\", \"industry\", \"charles\", \"no\", \"rooms\", \"age\", \"distance\", \"radial\",\n",
    "          \"tax\", \"pupil\", \"aam\", \"lower\", \"med_price\"]\n",
    "boston_data = pd.read_csv(\"datasets/XGBoost/housing.csv\", names=names, sep='\\s+')\n",
    "X, y = boston_data.iloc[:, :-1], boston_data.iloc[:,-1]\n",
    "boston_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Parametros a tunnear\n",
    "gbm_param_grid = {\"learning_rate\": [0.01, 0.1, 0.5, 0.9],\n",
    "                  \"n_estimators\": [200], \"subsample\": [0.3, 0.5, 0.9]}\n",
    "\n",
    "gbm = xgb. XGBRegressor()\n",
    "grid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid,\n",
    "                        scoring=\"neg_mean_squared_error\", cv=4, verbose=0)\n",
    "grid_mse.fit(X, y)\n",
    "\n",
    "best_params = grid_mse.best_params_\n",
    "best_score = grid_mse.best_score_\n",
    "\n",
    "print(f\"Mejores parametros {best_params}\")\n",
    "print(f\"Mejor score {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Search permite crear un conjunto infinito de hiperparametros y luego se definen el numero de iteraciones en las que la busqueda del modelo optimo continua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada una de las iteraciones se selecciona un valor random de los hiperparametros y se recalcula el modelo con eso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se alcanza el máximo de iteraciones antes fijado, simplemente se escoge la que tuvo el mejor resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parametros {'subsample': 0.44999999999999996, 'n_estimators': 200, 'learning_rate': 0.060000000000000005}\n",
      "Mejor score -18.867646521373217\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Load data\n",
    "names =  [\"crime\", \"zone\", \"industry\", \"charles\", \"no\", \"rooms\", \"age\", \"distance\", \"radial\",\n",
    "          \"tax\", \"pupil\", \"aam\", \"lower\", \"med_price\"]\n",
    "boston_data = pd.read_csv(\"datasets/XGBoost/housing.csv\", names=names, sep='\\s+')\n",
    "X, y = boston_data.iloc[:, :-1], boston_data.iloc[:,-1]\n",
    "boston_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Parametros a tunnear\n",
    "gbm_param_grid = {\"learning_rate\": np.arange(0.01, 0.9, .05),\n",
    "                  \"n_estimators\": [200], \"subsample\": np.arange(0.3, 0.9, .05)}\n",
    "\n",
    "gbm = xgb. XGBRegressor()\n",
    "grid_mse = RandomizedSearchCV(estimator=gbm, param_distributions=gbm_param_grid,\n",
    "                              scoring=\"neg_mean_squared_error\", cv=4, verbose=0,\n",
    "                              n_iter=25)\n",
    "grid_mse.fit(X, y)\n",
    "\n",
    "best_params = grid_mse.best_params_\n",
    "best_score = grid_mse.best_score_\n",
    "\n",
    "print(f\"Mejores parametros {best_params}\")\n",
    "print(f\"Mejor score {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost puede ser incorporado en una pipeline de scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las pipelines reciben una lista de tuplas como input, en donde el primer elemento de cda tupla es el en nombre del paso y luego el segundo elemento es el objeto con el que se realiza la operación (debe ser compatible con scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE final: 4.17692044815858\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "names =  [\"crime\", \"zone\", \"industry\", \"charles\", \"no\", \"rooms\", \"age\", \"distance\", \"radial\",\n",
    "          \"tax\", \"pupil\", \"aam\", \"lower\", \"med_price\"]\n",
    "boston_data = pd.read_csv(\"datasets/XGBoost/housing.csv\", names=names, sep='\\s+')\n",
    "X, y = boston_data.iloc[:, :-1], boston_data.iloc[:,-1]\n",
    "\n",
    "rf_pipeline = Pipeline([(\"st_scaler\", StandardScaler()),\n",
    "                        (\"rf_model\", RandomForestRegressor())])\n",
    "\n",
    "scores = cross_val_score(rf_pipeline, X, y, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "final_avg_rsme = np.mean(np.sqrt(np.abs(scores)))\n",
    "\n",
    "print(f\"RMSE final: {final_avg_rsme}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1- LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convierte columnas categoricas en enteros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv(\"datasets/XGBoost/ames_unprocessed_data.csv\", sep=',')\n",
    "\n",
    "# Rellenamos los valores vacios\n",
    "df.LotFrontage = df.LotFrontage.fillna(0)\n",
    "\n",
    "# Buscamos las columnas que son categoricas\n",
    "categorical_mask = (df.dtypes == object)\n",
    "\n",
    "# Una vez que sabemos cuales son categoricas y cuales no, creamos una lista solo con categoricas\n",
    "categorical_columns = df.columns[categorical_mask].tolist()\n",
    "\n",
    "# Creamos el objeto de LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Aplicamos LabelEncoder en cada una de las columnas categoricas\n",
    "df[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2- OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convierte una columna categorica en multiples columnas binarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma del dataframe original (1460, 21)\n",
      "Forma del array resultante (1460, 62)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "df = pd.read_csv(\"datasets/XGBoost/ames_unprocessed_data.csv\", sep=',')\n",
    "\n",
    "# Rellenamos los valores vacios\n",
    "df.LotFrontage = df.LotFrontage.fillna(0)\n",
    "\n",
    "# Buscamos las columnas que son categoricas\n",
    "categorical_mask = (df.dtypes == object)\n",
    "\n",
    "# Una vez que sabemos cuales son categoricas y cuales no, creamos una lista solo con categoricas\n",
    "categorical_columns = df.columns[categorical_mask].tolist()\n",
    "\n",
    "# Creamos el objeto de OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Usamos ColumnTransformer para especificar las columnas en que queremos aplicarlo\n",
    "column_trans = ColumnTransformer([(\"onehot_categorical\", ohe, categorical_columns)],\n",
    "                                 remainder=\"passthrough\")\n",
    "df_encoded = column_trans.fit_transform(df)\n",
    "\n",
    "print(\"Forma del dataframe original\", df.shape)\n",
    "\n",
    "# Print the shape of the transformed array\n",
    "print(\"Forma del array resultante\", df_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3- DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ni LabelEncoder ni OneHotEncoder se pueden utilizar en una pipeline, sin embargo, DictVectorizer nos permite hacer lo mismo en una sola linea y se puede incorporar a una Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es usualmente usado en procesamiento de texto. Busca convertir listas de caracteristicas en vectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que necesitamos es convertir edataframe de pandas en una lista de dcicionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 2.000e+00 5.480e+02 1.710e+03 1.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n",
      "  8.450e+03 6.500e+01 6.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00 7.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 2.085e+05 2.003e+03]\n",
      " [3.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  1.000e+00 1.000e+00 2.000e+00 4.600e+02 1.262e+03 0.000e+00 0.000e+00\n",
      "  0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  9.600e+03 8.000e+01 2.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 8.000e+00 6.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 1.815e+05 1.976e+03]\n",
      " [3.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 1.000e+00 2.000e+00 6.080e+02 1.786e+03 1.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n",
      "  1.125e+04 6.800e+01 6.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00 7.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 1.000e+00 2.235e+05 2.001e+03]\n",
      " [3.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 1.000e+00 1.000e+00 6.420e+02 1.717e+03 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n",
      "  9.550e+03 6.000e+01 7.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00 7.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 1.000e+00 1.400e+05 1.915e+03]\n",
      " [4.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 1.000e+00 2.000e+00 8.360e+02 2.198e+03 1.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n",
      "  1.426e+04 8.400e+01 6.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00 8.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 2.500e+05 2.000e+03]]\n",
      "{'MSSubClass': 23, 'MSZoning=RL': 27, 'LotFrontage': 22, 'LotArea': 21, 'Neighborhood=CollgCr': 34, 'BldgType=1Fam': 1, 'HouseStyle=2Story': 18, 'OverallQual': 55, 'OverallCond': 54, 'YearBuilt': 61, 'Remodeled': 59, 'GrLivArea': 11, 'BsmtFullBath': 6, 'BsmtHalfBath': 7, 'FullBath': 9, 'HalfBath': 12, 'BedroomAbvGr': 0, 'Fireplaces': 8, 'GarageArea': 10, 'PavedDrive=Y': 58, 'SalePrice': 60, 'Neighborhood=Veenker': 53, 'HouseStyle=1Story': 15, 'Neighborhood=Crawfor': 35, 'Neighborhood=NoRidge': 44, 'Neighborhood=Mitchel': 40, 'HouseStyle=1.5Fin': 13, 'Neighborhood=Somerst': 50, 'Neighborhood=NWAmes': 43, 'MSZoning=RM': 28, 'Neighborhood=OldTown': 46, 'Neighborhood=BrkSide': 32, 'BldgType=2fmCon': 2, 'HouseStyle=1.5Unf': 14, 'Neighborhood=Sawyer': 48, 'Neighborhood=NridgHt': 45, 'Neighborhood=NAmes': 41, 'BldgType=Duplex': 3, 'Neighborhood=SawyerW': 49, 'Neighborhood=IDOTRR': 38, 'PavedDrive=N': 56, 'Neighborhood=MeadowV': 39, 'BldgType=TwnhsE': 5, 'MSZoning=C (all)': 24, 'Neighborhood=Edwards': 36, 'Neighborhood=Timber': 52, 'PavedDrive=P': 57, 'HouseStyle=SFoyer': 19, 'MSZoning=FV': 25, 'Neighborhood=Gilbert': 37, 'HouseStyle=SLvl': 20, 'BldgType=Twnhs': 4, 'Neighborhood=StoneBr': 51, 'HouseStyle=2.5Unf': 17, 'Neighborhood=ClearCr': 33, 'Neighborhood=NPkVill': 42, 'HouseStyle=2.5Fin': 16, 'Neighborhood=Blmngtn': 29, 'Neighborhood=BrDale': 31, 'Neighborhood=SWISU': 47, 'MSZoning=RH': 26, 'Neighborhood=Blueste': 30}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "df = pd.read_csv(\"datasets/XGBoost/ames_unprocessed_data.csv\", sep=',')\n",
    "\n",
    "# Rellenamos los valores vacios\n",
    "df.LotFrontage = df.LotFrontage.fillna(0)\n",
    "\n",
    "# Convertimos el dataframe a diccionario\n",
    "df_dict = df.to_dict(orient='records')\n",
    "\n",
    "# Creamos el objeto DictVectorizer\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "# Aplicamos la tranformación\n",
    "df_encoded = dv.fit_transform(df_dict)\n",
    "\n",
    "# Imprimimos el resultado para las 5 primeras filas\n",
    "print(df_encoded[:5,:])\n",
    "\n",
    "# Imprimimos el resultado generado para el vocabulario\n",
    "print(dv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('ohe_onestep', DictVectorizer(sparse=False)),\n",
       "                ('xgb_model',\n",
       "                 XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                              colsample_bylevel=1, colsample_bynode=1,\n",
       "                              colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "                              importance_type='gain',\n",
       "                              interaction_constraints='',\n",
       "                              learning_rate=0.300000012, max_delta_step=0,\n",
       "                              max_depth=6, min_child_weight=1, missing=nan,\n",
       "                              monotone_constraints='()', n_estimators=100,\n",
       "                              n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "                              reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "                              subsample=1, tree_method='exact',\n",
       "                              validate_parameters=1, verbosity=None))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "df = pd.read_csv(\"datasets/XGBoost/ames_unprocessed_data.csv\", sep=',')\n",
    "\n",
    "X, y = df.iloc[:, :-1], df.iloc[:,-1]\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "X.LotFrontage = X.LotFrontage.fillna(0)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
    "         (\"xgb_model\", xgb.XGBRegressor())]\n",
    "\n",
    "# Create the pipeline: xgb_pipeline\n",
    "xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "# Fit the pipeline\n",
    "xgb_pipeline.fit(X.to_dict('records'), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5- Componentes adicionales para Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn_pandas: Intenta unir scikit-learn con pandas para aquellos casos en que no funcionan bien juntas. \n",
    "- Tiene una clase llamada DataFrameMapper que permite convertir rapidamente de dataframe a el formato que scikit-learn necesita \n",
    "- CategoricalImputer permite imputar valores a las variables categoricas antes de que se conviertan en enteros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn.preprossing.Imputer: Imputar valores a columnas numericas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn.pipeline.FeatureUnion: Permite combinar multiples pipelines de caracteristicas en solo una"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold RMSE:  27683.04157118635\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "df = pd.read_csv(\"datasets/XGBoost/ames_unprocessed_data.csv\", sep=',')\n",
    "\n",
    "X, y = df.iloc[:, :-1], df.iloc[:,-1]\n",
    "\n",
    "X.LotFrontage = X.LotFrontage.fillna(0)\n",
    "\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
    "         (\"xgb_model\", xgb.XGBRegressor(max_depth=2, objective=\"reg:linear\"))]\n",
    "\n",
    "xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "cross_val_scores = cross_val_score(xgb_pipeline, X.to_dict('records'), y,\n",
    "                                   scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "print(\"10-fold RMSE: \", np.mean(np.sqrt(np.abs(cross_val_scores))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
