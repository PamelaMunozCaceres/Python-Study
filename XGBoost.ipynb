{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que hace tan popular a XGBoost es la velocidad y los resultados que logra alcanzar. El algoritmos es paralelizable y por ende logra ser rápido de entrenar, se puede paralelizar en la GPU y entre una red de computadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost ha logrado una performance al nivel del estado del arte en muchas tareas de ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Clasificación con XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos el dataset de cancer de mamas (de ML con árboles de decisión) para crear un modelo de XGBoost rápido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "# Traemos los datos\n",
    "wbc = pd.read_csv(\"datasets/MLTreeModels/wbc.csv\")\n",
    "# Reemplazamos por 1 y 0\n",
    "wbc['diagnosis'] = wbc['diagnosis'].replace(['M', 'B'], [1, 0])\n",
    "X = wbc[['radius_mean', 'texture_mean', 'perimeter_mean',\n",
    "         'area_mean', 'smoothness_mean', 'compactness_mean',\n",
    "         'concavity_mean', 'concave points_mean', 'symmetry_mean',\n",
    "         'fractal_dimension_mean', 'radius_se', 'texture_se',\n",
    "         'perimeter_se', 'area_se', 'smoothness_se',\n",
    "         'compactness_se', 'concavity_se', 'concave points_se',\n",
    "         'symmetry_se', 'fractal_dimension_se', 'radius_worst',\n",
    "         'texture_worst', 'perimeter_worst', 'area_worst',\n",
    "         'smoothness_worst', 'compactness_worst', 'concavity_worst',\n",
    "         'concave points_worst', 'symmetry_worst',\n",
    "         'fractal_dimension_worst']]\n",
    "y = wbc[[\"diagnosis\"]]\n",
    "\n",
    "# Dividimos los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                   random_state=123)\n",
    "\n",
    "# Instanciamos el objeto\n",
    "xg_cl = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                          n_estimators=10, seed=123)\n",
    "\n",
    "# Ajustamos a los datos de entrenamiento\n",
    "xg_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predecimos\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "# Evaluamos\n",
    "score = accuracy_score(y_test, preds)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost es un ensamble algorithm ya que usa el resultado de muchos otros modelos para poder predecir. Usa los árboles de decisión como base learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting se puede entender como un concepto que se aplica a un set de modelos de machine learning, es un algoritmo de ensamble. Se usa para convertir un conjunto de weak learners en un strong learner (cualquier algoritmo que pueda ser perfeccionado para lograr una mejor performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation en XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usar cross-validation directamente con xgboost debemos transformar la data en un formato especifico llamado DMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9525\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Traemos los datos\n",
    "wbc = pd.read_csv(\"datasets/MLTreeModels/wbc.csv\")\n",
    "# Reemplazamos por 1 y 0\n",
    "wbc['diagnosis'] = wbc['diagnosis'].replace(['M', 'B'], [1, 0])\n",
    "X = wbc[['radius_mean', 'texture_mean', 'perimeter_mean',\n",
    "         'area_mean', 'smoothness_mean', 'compactness_mean',\n",
    "         'concavity_mean', 'concave points_mean', 'symmetry_mean',\n",
    "         'fractal_dimension_mean', 'radius_se', 'texture_se',\n",
    "         'perimeter_se', 'area_se', 'smoothness_se',\n",
    "         'compactness_se', 'concavity_se', 'concave points_se',\n",
    "         'symmetry_se', 'fractal_dimension_se', 'radius_worst',\n",
    "         'texture_worst', 'perimeter_worst', 'area_worst',\n",
    "         'smoothness_worst', 'compactness_worst', 'concavity_worst',\n",
    "         'concave points_worst', 'symmetry_worst',\n",
    "         'fractal_dimension_worst']]\n",
    "y = wbc[[\"diagnosis\"]]\n",
    "churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "params = {\"objective\": \"binary:logistic\", \"max_depth\":4}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=4,\n",
    "                    num_boost_round=10, metrics=\"error\", as_pandas=True)\n",
    "\n",
    "performance = round((1 - cv_results[\"test-error-mean\"]).iloc[-1], 4)\n",
    "print(f\"Accuracy: {performance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost es útil cuando se tienen grandes cantidades de datos (aunque basta con el numero de caracteristicas sea menor al numero de filas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciona bien cuando hay una mezcla de variables categoricas y numericas, o cuando son solo numericas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No es un buen algoritomo cuandos se trata de reconocimiento de imagenes, computer vision o NLP (para lo que es mejor usar Deep Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Regresión con XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predecir valores continuos, en la mayoría de los casos se usa RMSE como métrica para medire la performanca, aunque tambien se usa frecuentemente MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Funciones objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miden que tan lejos estan las predicciones de los valores reales, el objetivo es encontrar el modelo que minimice el valor de la funcion objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En XGBoost las funciones objetivo mas comunes son reg:linear (para problemas de regresión), reg:logistic (cuando se busca la clasificación de un registro) y binary:logistic (cuando se quiere saber la probabilidad de un suceso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Base Learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost combina muchos modelos individuales para poder predecir, cada uno de esos modelos individuales se conoce como base learner. Lo que se busca es que estos base learner sean un poco mejor que un modelo aleatorio en predecir una parte especifica del dataset y uniformamente malos en predecir lo demás"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RM</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.575</td>\n",
       "      <td>4.98</td>\n",
       "      <td>15.3</td>\n",
       "      <td>504000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.421</td>\n",
       "      <td>9.14</td>\n",
       "      <td>17.8</td>\n",
       "      <td>453600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.185</td>\n",
       "      <td>4.03</td>\n",
       "      <td>17.8</td>\n",
       "      <td>728700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.998</td>\n",
       "      <td>2.94</td>\n",
       "      <td>18.7</td>\n",
       "      <td>701400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.147</td>\n",
       "      <td>5.33</td>\n",
       "      <td>18.7</td>\n",
       "      <td>760200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      RM  LSTAT  PTRATIO      MEDV\n",
       "0  6.575   4.98     15.3  504000.0\n",
       "1  6.421   9.14     17.8  453600.0\n",
       "2  7.185   4.03     17.8  728700.0\n",
       "3  6.998   2.94     18.7  701400.0\n",
       "4  7.147   5.33     18.7  760200.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "boston_data = pd.read_csv(\"datasets/XGBoost/housing.csv\")\n",
    "boston_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = boston_data.iloc[:, :-1], boston_data.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=123)\n",
    "\n",
    "xg_reg = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=10,\n",
    "                          seed=123)\n",
    "\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "preds = xg_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 64703.978919794106\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función objetivo no solo nos indica que tan bueno es el modelo, sino que tambien que tan complejo es. Se le dice regularizacion a la idea de penalizar los modelos mientras estos se van volviendo complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea es obtener un modelo preciso, pero a la vez lo más simple posible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos de los parametros que permite aplicar regularización en XGBoost son:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma: Controla si un nodo se va a dividir o no basado en la reducción esperada de la función objetivo luego de hacer la división.  Mientras mayor es gamma, menos divisiones debiesen hacerse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha: L1 Regularization, penaliza el peso de las hojas. Mientras mayor el valor, mayor sera la regularizacion y el peso de las hojas tenderá a cero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda: L2 Regularization. Es una penalización un poco más sueve que la de L1. LLevan a los pesos de las hojas a descender suavemente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best rmse as a function of l1:\n",
      "    l1          rmse\n",
      "0    1  69886.398438\n",
      "1   10  70033.662110\n",
      "2  100  69887.656250\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "boston_data = pd.read_csv(\"datasets/XGBoost/housing.csv\")\n",
    "X, y = boston_data.iloc[:, :-1], boston_data.iloc[:,-1]\n",
    "boston_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "params = {\"objetive\": \"reg:linear\", \"max_depth\":4, \"verbosity\":0}\n",
    "\n",
    "l1_params = [1, 10, 100]\n",
    "rmses_l1 = []\n",
    "\n",
    "for reg in l1_params:\n",
    "    params[\"alpha\"] = reg\n",
    "    cv_results = xgb.cv(dtrain=boston_dmatrix, params=params, nfold=4,\n",
    "                        num_boost_round=10, metrics=\"rmse\", as_pandas=True,\n",
    "                        seed=123)\n",
    "    rmses_l1.append(cv_results[\"test-rmse-mean\"].tail(1).values[0])\n",
    "    \n",
    "print(\"Best rmse as a function of l1:\")\n",
    "print(pd.DataFrame(list(zip(l1_params, rmses_l1)), columns=[\"l1\", \"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Fine-Tunining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos como sería un entrenamiento sin optimizar los parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El RMSE del test es 71467.4501955\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "boston_data = pd.read_csv(\"datasets/XGBoost/housing.csv\")\n",
    "X, y = boston_data.iloc[:, :-1], boston_data.iloc[:,-1]\n",
    "boston_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "params = {\"objetive\": \"reg:linear\", \"verbosity\":0}\n",
    "\n",
    "# cross validation\n",
    "cv_results = xgb.cv(dtrain=boston_dmatrix, params=params, nfold=4,\n",
    "                    metrics=\"rmse\", as_pandas=True,\n",
    "                    seed=123)\n",
    "\n",
    "# Evaluacion\n",
    "rmse_test = cv_results[\"test-rmse-mean\"].tail(1).values[0]\n",
    "print(f\"El RMSE del test es {rmse_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora probamos con un modelo modificando algunos parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El RMSE del test es 69833.353516\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "boston_data = pd.read_csv(\"datasets/XGBoost/housing.csv\")\n",
    "X, y = boston_data.iloc[:, :-1], boston_data.iloc[:,-1]\n",
    "boston_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "params = {\"objective\": \"reg:linear\", \"verbosity\":0, \"colsample_bytree\": 0.3,\n",
    "          \"learning_rate\": 0.03, \"max_depth\": 4}\n",
    "\n",
    "# cross validation\n",
    "cv_results = xgb.cv(dtrain=boston_dmatrix, params=params, nfold=4,\n",
    "                    metrics=\"rmse\", as_pandas=True, num_boost_round=200,\n",
    "                    seed=123)\n",
    "\n",
    "# Evaluacion\n",
    "rmse_test = cv_results[\"test-rmse-mean\"].tail(1).values[0]\n",
    "print(f\"El RMSE del test es {rmse_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteramos sobre una lista de posibilidades para encontrar el mejor rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_boost_round           rmse\n",
      "0                5  420383.020833\n",
      "1               10  366499.052083\n",
      "2               15  320047.218750\n",
      "3               17  303012.729167\n",
      "4               19  287388.375000\n",
      "5               20  279842.708333\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "boston_data = pd.read_csv(\"datasets/XGBoost/housing.csv\")\n",
    "X, y = boston_data.iloc[:, :-1], boston_data.iloc[:,-1]\n",
    "boston_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "params = {\"objective\": \"reg:linear\", \"verbosity\":0, \"colsample_bytree\": 0.3,\n",
    "          \"learning_rate\": 0.03, \"max_depth\": 4}\n",
    "\n",
    "# Definimos distintas posibilidades de num_boost_round\n",
    "num_rounds = [5, 10, 15, 17, 19, 20]\n",
    "\n",
    "# Iteramos por cada posible combinacion\n",
    "final_rmse_per_round = []\n",
    "for n_rounds in num_rounds:\n",
    "    cv_results = xgb.cv(dtrain=boston_dmatrix, params=params, nfold=3,\n",
    "                        num_boost_round=n_rounds, metrics=\"rmse\",\n",
    "                        as_pandas=True, seed=123)\n",
    "    # Agregamos a la lista de resultados\n",
    "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "    \n",
    "# Unimos los resultados\n",
    "num_rounds_rmses = pd.DataFrame(list(zip(num_rounds, final_rmse_per_round)),\n",
    "                                columns=[\"num_boost_round\", \"rmse\"])\n",
    "print(num_rounds_rmses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Tunable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para modelos basados en arboles se pueden ajustar los siguientes parametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - learning rate: Que tan rápido se ajusta el modelo al error residual\n",
    "    - gamma: Cual es el minimo de ganacia en la funcion objetivo para que un nodo se divida\n",
    "    - lambda: Regularización L2\n",
    "    - alpha: Regularización L1\n",
    "    - max_depth: que tanto puede crecer un arbol en las rondas de boosting\n",
    "    - subsample: (va entre 0 y 1) El porcentaje de datos que se utilizará para entrenar el modelo en cada ronda de Boosting\n",
    "    - colsample_bytree: Fracción de caracteristicas que se pueden usar en cada ronda de boosting (es un valor entre 0 y 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para modelos basados en regresiones lineales se puede modificar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - lambda: Regularización L2\n",
    "    - alpha: Regularización L1\n",
    "    - lambda_bias: L2 en bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ambos es posible modificar el numero de estimadores que se usan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 GridSearch y RandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como escogemos la mejor combinación de parametros cuando estos interactuan en relaciones no lineales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search es un método que busca exhaustivamente sobre un set de hiperparametros, esto quiere decir, que el numero de modelos calculados será igual a la suma de los productos de el numero de hiperparametros a optimizar por la cantidad de opciones en cada uno de estos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parametros {'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.5}\n",
      "Mejor score -7224943874.651733\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load data\n",
    "boston_data = pd.read_csv(\"datasets/XGBoost/housing.csv\")\n",
    "X, y = boston_data.iloc[:, :-1], boston_data.iloc[:,-1]\n",
    "boston_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Parametros a tunnear\n",
    "gbm_param_grid = {\"learning_rate\": [0.01, 0.1, 0.5, 0.9],\n",
    "                  \"n_estimators\": [200], \"subsample\": [0.3, 0.5, 0.9]}\n",
    "\n",
    "gbm = xgb. XGBRegressor()\n",
    "grid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid,\n",
    "                        scoring=\"neg_mean_squared_error\", cv=4, verbose=0)\n",
    "grid_mse.fit(X, y)\n",
    "\n",
    "best_params = grid_mse.best_params_\n",
    "best_score = grid_mse.best_score_\n",
    "\n",
    "print(f\"Mejores parametros {best_params}\")\n",
    "print(f\"Mejor score {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Search permite crear un conjunto infinito de hiperparametros y luego se definen el numero de iteraciones en las que la busqueda del modelo optimo continua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada una de las iteraciones se selecciona un valor random de los hiperparametros y se recalcula el modelo con eso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se alcanza el máximo de iteraciones antes fijado, simplemente se escoge la que tuvo el mejor resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parametros {'subsample': 0.44999999999999996, 'n_estimators': 200, 'learning_rate': 0.060000000000000005}\n",
      "Mejor score -6795928543.038411\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Load data\n",
    "boston_data = pd.read_csv(\"datasets/XGBoost/housing.csv\")\n",
    "X, y = boston_data.iloc[:, :-1], boston_data.iloc[:,-1]\n",
    "boston_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Parametros a tunnear\n",
    "gbm_param_grid = {\"learning_rate\": np.arange(0.01, 0.9, .05),\n",
    "                  \"n_estimators\": [200], \"subsample\": np.arange(0.3, 0.9, .05)}\n",
    "\n",
    "gbm = xgb. XGBRegressor()\n",
    "grid_mse = RandomizedSearchCV(estimator=gbm, param_distributions=gbm_param_grid,\n",
    "                              scoring=\"neg_mean_squared_error\", cv=4, verbose=0,\n",
    "                              n_iter=25)\n",
    "grid_mse.fit(X, y)\n",
    "\n",
    "best_params = grid_mse.best_params_\n",
    "best_score = grid_mse.best_score_\n",
    "\n",
    "print(f\"Mejores parametros {best_params}\")\n",
    "print(f\"Mejor score {best_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
