{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apuntes del curso *Unit Testing for Data Science in Python* de Datacamp (https://learn.datacamp.com/courses/unit-testing-for-data-science-in-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al momento de definir una función es necesario testear si funciona de la manera en que se espera. La forma habitual es entregar parametros a la función de manera manual, ejecutarla y reconocer si el resultado es correcto o no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lamentablemente, ese proceso es largo y por cada modificación que se hace a la función debiesemos volver a verificar que el resultado es lo que esperamos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea detrás de los test unitarios es automatizar aquel proceso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En este Jupyter notebook usaré una funcionalidad que me permite importar el código desde un archivo .py (%load filename.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para ejecutar comandos de terminal en jupyter notebook basta anteponer un ! al comando, por ende todas las celdas que lleven un ! antes son comandos de terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Conceptos básicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay mas de una libreria en Python que permite realizar tests:\n",
    "        - pytest\n",
    "        - unittest\n",
    "        - nosetest\n",
    "        - doctest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La más popular es pytest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 -Para que pytest funcione, debemos crear un archivo .py que inicie con la palabra test_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- En ese archivo, importamos la libreria e importamos la función que necesitamos testear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- La función que se encarga de realizar el testeo tambien debe empezar con el nombre test_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- La verificación de que se cumple lo esperado se realiza mediante el statement assert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ende el test_module.py queda de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load test_module.py\n",
    "import pytest\n",
    "from utils_module import function_1\n",
    "\n",
    "def test_first_function():\n",
    "    assert function_1 == 1\n",
    "    \n",
    "def test_first_function_bad():\n",
    "    assert function_1 == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mientras que la función se encuentra en *utils_module.py* y lo unico que retorna es 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load utils_module.py\n",
    "def function_1():\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar los test debemos ingresar en la consola *pytest nombrearchivo.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.3, pytest-5.4.3, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: C:\\Users\\pamel\\Documents\\git\\Python-Study\\Unit testing\n",
      "collected 2 items\n",
      "\n",
      "test_module.py .F                                                        [100%]\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "___________________________ test_first_function_bad ___________________________\n",
      "\n",
      "    def test_first_function_bad():\n",
      ">       assert function_1() == 0\n",
      "E       assert 1 == 0\n",
      "E        +  where 1 = function_1()\n",
      "\n",
      "test_module.py:8: AssertionError\n",
      "=========================== short test summary info ===========================\n",
      "FAILED test_module.py::test_first_function_bad - assert 1 == 0\n",
      "========================= 1 failed, 1 passed in 0.13s =========================\n"
     ]
    }
   ],
   "source": [
    "!pytest test_module.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que obtenemos es el reporte de resultados del testeo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera parte describe información del sistema, la vesión de Python, entre otros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La segunda parte dice: *collected 2 items* esto quiere decir que hay 2 pruebas por hacer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, tenemos el nombre del modulo de testeo seguido por un *.F*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La F inidica que hubo un fallo, es decir, que no se cumplió lo que se esperaba. de igual fomar, si la ejecución falla por cualquier otra razon, tambien devolverá que el test falló."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El punto indica que la prueba fue existosa, por eso recibimos un *.F* pues la primera prueba fue exitosa y la segunda no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la sección de FAILURES tenemos una vista más detallada de lo que no funcionó"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente la ultima sección nos entrega un sumario de la ejecución, incluyendo el tiempo que tomó y las pruebas que no pasaron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Test como documentación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al tener pruebas programadas, tambien ayudamos a mejorar la documentación del sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verificar los test asociados a un proyecto basta con usar el comando *cat nombrearchivo.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nuestro caso: (como es windows uso type en lugar de cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pytest\n",
      "from utils_module import function_1\n",
      "\n",
      "def test_first_function():\n",
      "    assert function_1() == 1\n",
      "    \n",
      "def test_first_function_bad():\n",
      "    assert function_1() == 0\n"
     ]
    }
   ],
   "source": [
    "!type test_module.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Reducir el downtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al hacer test podemos disminuir significativamente los errores en productivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de enviar el codigo a los sistemas en producción podemos verificar si cumple los test y de no ser así rechazar los cambio y no cambiar los sistemas productivos, e incluso informar a los responsables que el codigo necesita modificaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tipos de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit test: verifican que una unidad este funcionando correctamente. Se conoce como una unidad a pequenas e independientes partes del codigo (puede ser una función o una clase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integration test: chequean si múltiples unidades funcionan bien cuando estan conectadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End to End test: verifican que todo el software en su conjunto funcione de la manera esperada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Agregar mensajes opcionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se usa el statement de assert podemos agregar un mensaje en el caso de que la prueba no haya sido exitosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se usa el siguiente formato:\n",
    "\n",
    "    assert function_result == expected_result, fail_message\n",
    "    assert a == b, \"a is not equal to b\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una función para calcular el área que contiene errores pues no convierte en float antes de hacer el calculo *compute_area_with_bug*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load utils_module.py\n",
    "def function_1():\n",
    "    return 1\n",
    "\n",
    "\n",
    "def compute_area_with_bug(a, b) -> float:\n",
    "    \"\"\"\n",
    "    calculate the area of a rectangle\n",
    "    \"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al test le agregamos el mensaje en el formato indicado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load test_module.py\n",
    "import pytest\n",
    "from utils_module import \\\n",
    "    (function_1, compute_area_with_bug)\n",
    "\n",
    "\n",
    "def test_first_function():\n",
    "    \"\"\"\n",
    "    this test should be successful\n",
    "    \"\"\"\n",
    "    assert function_1() == 1\n",
    "\n",
    "    \n",
    "def test_first_function_bad():\n",
    "    \"\"\"\n",
    "    this test should fail\n",
    "    \"\"\"\n",
    "    assert function_1() == 0\n",
    "    \n",
    "    \n",
    "def test_compute_area_bug():\n",
    "    \"\"\"\n",
    "    this test should fail if we pass a string it\n",
    "    won't transform it to float\n",
    "    \"\"\"\n",
    "    actual_value = compute_area_with_bug(2, \"3\")\n",
    "    expected_value = 6\n",
    "    msg = (f\"expected value is {expected_value},\"\n",
    "           f\" actual value is {actual_value}\")\n",
    "    assert actual_value == expected_value, msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente corremos el test, al igual que la vez anterior debiese fallar el segundo test, pero tambien el tercero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.3, pytest-5.4.3, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: C:\\Users\\pamel\\Documents\\git\\Python-Study\\Unit testing\n",
      "collected 3 items\n",
      "\n",
      "test_module.py .FF                                                       [100%]\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "___________________________ test_first_function_bad ___________________________\n",
      "\n",
      "    def test_first_function_bad():\n",
      ">       assert function_1() == 0\n",
      "E       assert 1 == 0\n",
      "E        +  where 1 = function_1()\n",
      "\n",
      "test_module.py:8: AssertionError\n",
      "____________________________ test_compute_area_bug ____________________________\n",
      "\n",
      "    def test_compute_area_bug():\n",
      "        actual_value = compute_area_with_bug(2, \"3\")\n",
      "        expected_value = 6\n",
      "        msg = (f\"expected value is {expected_value},\"\n",
      "               f\" actual value is {actual_value}\")\n",
      ">       assert actual_value == expected_value, msg\n",
      "E       AssertionError: expected value is 6, actual value is 33\n",
      "E       assert '33' == 6\n",
      "\n",
      "test_module.py:15: AssertionError\n",
      "=========================== short test summary info ===========================\n",
      "FAILED test_module.py::test_first_function_bad - assert 1 == 0\n",
      "FAILED test_module.py::test_compute_area_bug - AssertionError: expected value...\n",
      "========================= 2 failed, 1 passed in 0.14s =========================\n"
     ]
    }
   ],
   "source": [
    "!pytest test_module.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es recomendado incluir un mensake en statement de assert porque es más fácil de entender que el output automatico que entrega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Preacaución con las comparaciones de float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por la forma que trabaja python algunos calculos con float funcionan distinto a lo esperado, por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30000000000000004"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2 + 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ende obtendriamos un error si hiciesemos la comparación:\n",
    "\n",
    "    assert 0.2 + 0.1 == 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para solucionar este problema debemos usar la función pytest.approx(), donde el ejemplo anterior quedaría como:\n",
    "\n",
    "    assert 0.1 + 0.2 == pytest.approx(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING, se necesita hacer lo mismo con los numpy array\n",
    "\n",
    "    assert np.array([0.1, 0.1 + 0.2]) == pytest.approx(np.array([0.1, 0.3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Múltiples assertments por unit test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambien es posible realizar más de una prueba por cada unit test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos queremos saber si el resultado de la función es de un tipo en particular y agregamos el test test_compute_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las funciones quedan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load utils_module.py\n",
    "def function_1():\n",
    "    return 1\n",
    "\n",
    "\n",
    "def compute_area_with_bug(a, b) -> float:\n",
    "    \"\"\"\n",
    "    calculate the area of a rectangle\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def compute_area(a, b) -> float:\n",
    "    \"\"\"\n",
    "    calculate the area of a rectangle\n",
    "    \"\"\"\n",
    "    return float(a) * float(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los test quedan como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load test_module.py\n",
    "import pytest\n",
    "from utils_module import \\\n",
    "    (function_1, compute_area_with_bug, compute_area)\n",
    "\n",
    "\n",
    "def test_first_function():\n",
    "    \"\"\"\n",
    "    this test should be successful\n",
    "    \"\"\"\n",
    "    assert function_1() == 1\n",
    "\n",
    "    \n",
    "def test_first_function_bad():\n",
    "    \"\"\"\n",
    "    this test should fail\n",
    "    \"\"\"\n",
    "    assert function_1() == 0\n",
    "    \n",
    "    \n",
    "def test_compute_area_bug():\n",
    "    \"\"\"\n",
    "    this test should fail if we pass a string it\n",
    "    won't transform it to float\n",
    "    \"\"\"\n",
    "    actual_value = compute_area_with_bug(2, \"3\")\n",
    "    expected_value = 6\n",
    "    msg = (f\"expected value is {expected_value},\"\n",
    "           f\" actual value is {actual_value}\")\n",
    "    assert actual_value == expected_value, msg\n",
    "\n",
    "    \n",
    "def test_compute_area():\n",
    "    \"\"\"\n",
    "    this test should be successful\n",
    "    \"\"\"\n",
    "    actual_value = compute_area(2, \"3\")\n",
    "    expected_value = 6\n",
    "    msg = (f\"expected value is {expected_value},\"\n",
    "           f\" actual value is {actual_value}\")\n",
    "    assert isinstance(actual_value, float)\n",
    "    assert actual_value == expected_value, msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La única forma de que el test sea éxitoso es que ambos asstements de assert este correctos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.3, pytest-5.4.3, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: C:\\Users\\pamel\\Documents\\git\\Python-Study\\Unit testing\n",
      "collected 4 items\n",
      "\n",
      "test_module.py .FF.                                                      [100%]\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "___________________________ test_first_function_bad ___________________________\n",
      "\n",
      "    def test_first_function_bad():\n",
      "        \"\"\"\n",
      "        this test should fail\n",
      "        \"\"\"\n",
      ">       assert function_1() == 0\n",
      "E       assert 1 == 0\n",
      "E        +  where 1 = function_1()\n",
      "\n",
      "test_module.py:17: AssertionError\n",
      "____________________________ test_compute_area_bug ____________________________\n",
      "\n",
      "    def test_compute_area_bug():\n",
      "        \"\"\"\n",
      "        this test should fail if we pass a string it\n",
      "        won't transform it to float\n",
      "        \"\"\"\n",
      "        actual_value = compute_area_with_bug(2, \"3\")\n",
      "        expected_value = 6\n",
      "        msg = (f\"expected value is {expected_value},\"\n",
      "               f\" actual value is {actual_value}\")\n",
      ">       assert actual_value == expected_value, msg\n",
      "E       AssertionError: expected value is 6, actual value is 33\n",
      "E       assert '33' == 6\n",
      "\n",
      "test_module.py:29: AssertionError\n",
      "=========================== short test summary info ===========================\n",
      "FAILED test_module.py::test_first_function_bad - assert 1 == 0\n",
      "FAILED test_module.py::test_compute_area_bug - AssertionError: expected value...\n",
      "========================= 2 failed, 2 passed in 0.14s =========================\n"
     ]
    }
   ],
   "source": [
    "!pytest test_module.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como era de esperar el último test fue éxitoso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-Testeando excepciones en lugar de valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas funciones no retornan nada y en su lugar levantan una execepción cuando se le entregan argumentos especificos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea es probar que bajo ciertas condiciones esperamos que la función falle, pues lo que se le entrega como argumento no nos sirve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A diferencia de lo anterior en este caso el test será éxitoso si la función efectivamente falla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load utils_module_p2.py\n",
    "def full_name(user_info: dict) -> str:\n",
    "    '''\n",
    "    Join the first and last names of a specific\n",
    "    user to create the full name of this user\n",
    "    '''\n",
    "    return user_info['name'] + user_info['last_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función busca concatenar un nombre y apellido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load test_module_p2.py\n",
    "import pytest\n",
    "from utils_module_p2 import full_name\n",
    "\n",
    "\n",
    "def test_full_name_without_key():\n",
    "    '''\n",
    "    We want our full user name to have a first\n",
    "    and last name, so if it does not have it,\n",
    "    we expect the function to fail.\n",
    "    The test will succeed if the function\n",
    "    actually fails\n",
    "    '''\n",
    "    user = {'last_name': 'Munoz'}\n",
    "    with pytest.raises(KeyError) as exception_info:\n",
    "        full_name(user)\n",
    "    assert exception_info.match('name')\n",
    "        \n",
    "\n",
    "def test_full_name_with_list():\n",
    "    '''\n",
    "    we want our full name to be a string and not\n",
    "    a list, so if it ends up being a list,\n",
    "    we expect the function to fail.\n",
    "    The test will succeed if the function\n",
    "    actually fails\n",
    "    '''\n",
    "    user = {'last_name': ['Munoz'], 'name': ['Pamela']}\n",
    "    with pytest.raises(TypeError):\n",
    "        full_name(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer test debiese pasar pues la función es incapaz de crear el nombre completo si no tiene la llave adecuada para ello. Además, incluir el statement de assert al final nos permite conocer si el mensaje de error era efectivamente el que esperabamos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otra parte el segundo test fracasará pues en lugar de devolvernos un error de tipo (en el diccionario los valores son listas y no str) la función terminará su ejecución sin problemas entregando como resultado final una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.3, pytest-5.4.3, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: C:\\Users\\pamel\\Documents\\git\\Python-Study\\Unit testing\n",
      "collected 2 items\n",
      "\n",
      "test_module_p2.py .F                                                     [100%]\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "__________________________ test_full_name_with_list ___________________________\n",
      "\n",
      "    def test_full_name_with_list():\n",
      "        '''\n",
      "        we want our full name to be a string and not\n",
      "        a list, so if it ends up being a list,\n",
      "        we expect the function to fail.\n",
      "        The test will succeed if the function\n",
      "        actually fails\n",
      "        '''\n",
      "        user = {'last_name': ['Munoz'], 'name': ['Pamela']}\n",
      "        with pytest.raises(TypeError):\n",
      ">           full_name(user)\n",
      "E           Failed: DID NOT RAISE <class 'TypeError'>\n",
      "\n",
      "test_module_p2.py:29: Failed\n",
      "=========================== short test summary info ===========================\n",
      "FAILED test_module_p2.py::test_full_name_with_list - Failed: DID NOT RAISE <c...\n",
      "========================= 1 failed, 1 passed in 0.13s =========================\n"
     ]
    }
   ],
   "source": [
    "!pytest test_module_p2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6- ¿Cuanto tests son suficientes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que no podemos probar todas las combinaciones de argumentos que recibiría una función necesitamos enfocarnos en cuestiones especificas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo ideal es testear por:\n",
    "    \n",
    "    - Malos argumentos\n",
    "    - Argumentos especiales\n",
    "    - Argumentos normales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los argumentos malos son aquellos en los que la función levanta una excepción cuando los recibe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los argumentos especiales se pueden dividir en dos grupos, los valores límites y los que la función usa una lógica especial para ellos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores límites son todos aquellos en los que una vez alcanzados la función comienza a demostrar un comportamiento diferente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo lo que no sea un argumento malo o describa un comportamiento especial de la función se considera un argumento normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No todas las funciones tienen argumentos malos o especiales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7- Test Driven Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si asumimos que los test unitarios no son realmente necesarios en un principio, llegaremos al final del proyecto sin ningun test escrito y por ende ninguna forma de prevenir errores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea tras TDD es asegurar que los test efectivamente se escriban"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ciclo usual de creación de funciones es que en un principio la función se crea y luego se testea, si hay un fallo, este se arregla y en caso contrario la función se acepta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con TDD antes de que la función se cree, se escribe un test indicando lo que esta debe hacer y luego el mismo proceso usual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma no se posterga la creación de los test, el proceso de testeo se hace en conjunto con el desarrollo y ayuda a identificar argumentos especiales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8- ¿Como organizar un set de tests?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A medida que el código crece, tambien lo hace el número de test y por ende necesitamos una forma ordenada de organizarlos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo recomendado es crear un directorio para los test en el mismo nivel que la carpeta *src*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro de la carpeta tests, simplemente duplicamos la estructura de carpetas dentro de src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regla general indica que por cada modulo dentro de src debiese existir un modulo dentro de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tenemos el modulo utils/module.py en la carpeta src, debiesemos tener el modulo utils/test_module.py en el directorio tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1- Test class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de que el tengamos muchos test asociados a múltiples funciones en el mismo archivo y necesitemos saber donde empiezan los test de una función y termian los de otra podemos usar clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El nombre de la clase siempre debe estar en CamelCase y debe contener la palabra Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vease el ejemplo a continuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load test_module_p3.py\n",
    "import pytest\n",
    "from utils_module import \\\n",
    "    (function_1, compute_area_with_bug)\n",
    "\n",
    "class TestFunction1(object):\n",
    "\n",
    "    def test_first_function(self):\n",
    "        \"\"\"\n",
    "        this test should be successful\n",
    "        \"\"\"\n",
    "        assert function_1() == 1\n",
    "\n",
    "\n",
    "    def test_first_function_bad(self):\n",
    "        \"\"\"\n",
    "        this test should fail\n",
    "        \"\"\"\n",
    "        assert function_1(self) == 0\n",
    "\n",
    "class TestComputeAreaWithBug(object):\n",
    "\n",
    "    def test_compute_area_bug(self):\n",
    "        \"\"\"\n",
    "        this test should fail if we pass a string it\n",
    "        won't transform it to float\n",
    "        \"\"\"\n",
    "        actual_value = compute_area_with_bug(2, \"3\")\n",
    "        expected_value = 6\n",
    "        msg = (f\"expected value is {expected_value},\"\n",
    "               f\" actual value is {actual_value}\")\n",
    "        assert actual_value == expected_value, msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.3, pytest-5.4.3, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: C:\\Users\\pamel\\Documents\\git\\Python-Study\\Unit testing\n",
      "collected 3 items\n",
      "\n",
      "test_module_p3.py .FF                                                    [100%]\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "____________________ TestFunction1.test_first_function_bad ____________________\n",
      "\n",
      "self = <test_module_p3.TestFunction1 object at 0x0000028F7574AA30>\n",
      "\n",
      "    def test_first_function_bad(self):\n",
      "        \"\"\"\n",
      "        this test should fail\n",
      "        \"\"\"\n",
      ">       assert function_1(self) == 0\n",
      "E       TypeError: function_1() takes 0 positional arguments but 1 was given\n",
      "\n",
      "test_module_p3.py:18: TypeError\n",
      "________________ TestComputeAreaWithBug.test_compute_area_bug _________________\n",
      "\n",
      "self = <test_module_p3.TestComputeAreaWithBug object at 0x0000028F75771A00>\n",
      "\n",
      "    def test_compute_area_bug(self):\n",
      "        \"\"\"\n",
      "        this test should fail if we pass a string it\n",
      "        won't transform it to float\n",
      "        \"\"\"\n",
      "        actual_value = compute_area_with_bug(2, \"3\")\n",
      "        expected_value = 6\n",
      "        msg = (f\"expected value is {expected_value},\"\n",
      "               f\" actual value is {actual_value}\")\n",
      ">       assert actual_value == expected_value, msg\n",
      "E       AssertionError: expected value is 6, actual value is 33\n",
      "E       assert '33' == 6\n",
      "\n",
      "test_module_p3.py:31: AssertionError\n",
      "=========================== short test summary info ===========================\n",
      "FAILED test_module_p3.py::TestFunction1::test_first_function_bad - TypeError:...\n",
      "FAILED test_module_p3.py::TestComputeAreaWithBug::test_compute_area_bug - Ass...\n",
      "========================= 2 failed, 1 passed in 0.14s =========================\n"
     ]
    }
   ],
   "source": [
    "!pytest test_module_p3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 - Hacer pruebas a todo el proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder hacer pruebas a todo el proyecto en su conjunto  basta con ubicarse en el directorio test/ y escribir pytest en la terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automaticamente se reconocerán todos los archivos que comiencen con la palabra *test_*, se identificaran las clases que comiencen con la palabra *Test* y las funciones dentro que comiencen con *test_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si agregamos -x al comando pytest, es decir *pytest -x* la ejecución de los test se detandrá si es alguno falla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3- Hacer pruebas usando un patron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si solo queremos hacer pruebas a una funcionalidad especifica podemos incluir el argumento -k y luego el nombre de la clase que queremos testear. Podriamos escribir parte del nombre siempre y cuando esa parte sea única para la clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9- Fallos esperados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de que efectivamente usemos TDD y por ende escribamos el test de la función, antes que la función propiamente tal, al hacer correr el test, este fallará y por ende todo el testeo será negativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evitar que el resultado del test total sea negativo cuando sabemos que uno de los test va a fracas podemos usar el decorador @pytest.mark.xfail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando como práactica TDD, en primer lugar creamos una función que nos permita testear una futura función para calcular el área de un triangulo (ultima función)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load test_module_p4.py\n",
    "import pytest\n",
    "from utils_module import \\\n",
    "    (function_1, compute_area_with_bug, compute_area_triangle)\n",
    "\n",
    "class TestFunction1(object):\n",
    "\n",
    "    def test_first_function(self):\n",
    "        \"\"\"\n",
    "        this test should be successful\n",
    "        \"\"\"\n",
    "        assert function_1() == 1\n",
    "\n",
    "\n",
    "    def test_first_function_bad(self):\n",
    "        \"\"\"\n",
    "        this test should fail\n",
    "        \"\"\"\n",
    "        assert function_1(self) == 0\n",
    "\n",
    "class TestComputeAreaWithBug(object):\n",
    "\n",
    "    def test_compute_area_bug(self):\n",
    "        \"\"\"\n",
    "        this test should fail if we pass a string it\n",
    "        won't transform it to float\n",
    "        \"\"\"\n",
    "        actual_value = compute_area_with_bug(2, \"3\")\n",
    "        expected_value = 6\n",
    "        msg = (f\"expected value is {expected_value},\"\n",
    "               f\" actual value is {actual_value}\")\n",
    "        assert actual_value == expected_value, msg\n",
    "\n",
    "    @pytest.mark.xfail\n",
    "    def test_compute_area_triangle(self):\n",
    "\n",
    "        actual_value = compute_area_triangle(2, 3)\n",
    "        expected_value = 3\n",
    "        msg = (f\"expected value is {expected_value},\"\n",
    "               f\" actual value is {actual_value}\")\n",
    "        assert actual_value == expected_value, msg        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego incluimos tan solo el nombre de la función en el archivo a testear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load utils_module.py\n",
    "def function_1():\n",
    "    return 1\n",
    "\n",
    "\n",
    "def compute_area_with_bug(a, b) -> float:\n",
    "    \"\"\"\n",
    "    calculate the area of a rectangle\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def compute_area(a, b) -> float:\n",
    "    \"\"\"\n",
    "    calculate the area of a rectangle\n",
    "    \"\"\"\n",
    "    return float(a) * float(b)\n",
    "\n",
    "\n",
    "def compute_area_triangle(a, b) -> float:\n",
    "    \"\"\"\n",
    "    calculate the area of a triangle\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviamente el test va a fallar porque no hemos siquiera definido la función, pero como somos conscientes de aquello, no necesitamos que todo el modulo de pruebas fracase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.3, pytest-5.4.3, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: C:\\Users\\pamel\\Documents\\git\\Python-Study\\Unit testing\n",
      "collected 4 items\n",
      "\n",
      "test_module_p4.py .FFx                                                   [100%]\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "____________________ TestFunction1.test_first_function_bad ____________________\n",
      "\n",
      "self = <test_module_p4.TestFunction1 object at 0x000001686B57AC10>\n",
      "\n",
      "    def test_first_function_bad(self):\n",
      "        \"\"\"\n",
      "        this test should fail\n",
      "        \"\"\"\n",
      ">       assert function_1(self) == 0\n",
      "E       TypeError: function_1() takes 0 positional arguments but 1 was given\n",
      "\n",
      "test_module_p4.py:18: TypeError\n",
      "________________ TestComputeAreaWithBug.test_compute_area_bug _________________\n",
      "\n",
      "self = <test_module_p4.TestComputeAreaWithBug object at 0x000001686B5AB640>\n",
      "\n",
      "    def test_compute_area_bug(self):\n",
      "        \"\"\"\n",
      "        this test should fail if we pass a string it\n",
      "        won't transform it to float\n",
      "        \"\"\"\n",
      "        actual_value = compute_area_with_bug(2, \"3\")\n",
      "        expected_value = 6\n",
      "        msg = (f\"expected value is {expected_value},\"\n",
      "               f\" actual value is {actual_value}\")\n",
      ">       assert actual_value == expected_value, msg\n",
      "E       AssertionError: expected value is 6, actual value is 33\n",
      "E       assert '33' == 6\n",
      "\n",
      "test_module_p4.py:31: AssertionError\n",
      "=========================== short test summary info ===========================\n",
      "FAILED test_module_p4.py::TestFunction1::test_first_function_bad - TypeError:...\n",
      "FAILED test_module_p4.py::TestComputeAreaWithBug::test_compute_area_bug - Ass...\n",
      "=================== 2 failed, 1 passed, 1 xfailed in 0.19s ====================\n"
     ]
    }
   ],
   "source": [
    "!pytest test_module_p4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La X nos indica que el test saltó es función y no probó su funcionamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10- Más allá del assertion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas funciones necesitan de una precondición para funcionar, digase un archivo, una conexión a una base de datos, entre otras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mientras que el output de la función puede ser dejar un archivo u otro en el ambiente, como no queremos que afecte nuestros futuros test al terminar el test devolvemos el ambiente a su estado incial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ende, para probar la función debemos setear el ambiente, probarla y luego resetear el ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos queremos testear la función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load Project/src/readmodule/read.py\n",
    "\n",
    "def read_count(filename, character):\n",
    "    \"\"\"\n",
    "    Read a txt file and count letter\n",
    "    \"\"\"\n",
    "    # Open file\n",
    "    with open(filename, \"r\") as file:\n",
    "        text = file.read()\n",
    "    # Count the number occurrences of a character\n",
    "    number = text.count(character)\n",
    "    # Store the output in a file\n",
    "    with open(f\"character {character}.txt\", 'w') as f:\n",
    "        f.write(f\"{number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función primero lle un archivo (necesitamos que ese archivo exista), lo procesa y guarda otro archivo (debemos borrar una vez finalizado el test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder preparar el ambiente y luego borrar los archivos generados una vez hecho el test usamos la función *setup_data* que lleva como decorador *@pytest.fixture*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load Project/tests/readmodule/test_read.py\n",
    "import pytest\n",
    "import os\n",
    "from src.readmodule.read import read_count\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def setup_data():\n",
    "    path = 'file.txt'\n",
    "    character = 'a'\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(\"Hello my name is Pamela\")\n",
    "    yield path, character\n",
    "    os.remove(path)\n",
    "    os.remove(f\"character {character}.txt\")\n",
    "    \n",
    "\n",
    "    \n",
    "class TestReading(object):\n",
    "    \n",
    "    def test_read(self, setup_data):\n",
    "        path, character = setup_data\n",
    "        # Call the function\n",
    "        read_count(path, character)\n",
    "        # Check the result\n",
    "        with open(f\"character {character}.txt\", 'r') as f:\n",
    "            number = f.read()\n",
    "        assert int(float(number)) == 3, f'value is {int(number)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de invocar la funcion de test, que se invoca con un yield en vez de return, eliminamos los archivos generados del ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.3, pytest-5.4.3, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: C:\\Users\\pamel\\Documents\\git\\Python-Study\\Unit testing\\Project\n",
      "collected 1 item\n",
      "\n",
      "tests\\readmodule\\test_read.py .                                          [100%]\n",
      "\n",
      "============================== 1 passed in 0.03s ==============================\n"
     ]
    }
   ],
   "source": [
    "!cd Project && python -m pytest tests/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1- tmpdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tmpdir es una herramienta, incluida en pytest, que es útil para manejar archivos temporales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tmpdir crea un directorio temporal en la fase de setup para luego eliminarlo en la fase de teardown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La diferencia con el archivo de test anterior seria la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from src.readmodule.read import read_count\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def setup_data(tmpdir):\n",
    "    path = tmpdir.join('file.txt')\n",
    "    character = 'a'\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(\"Hello my name is Pamela\")\n",
    "    yield path, character\n",
    "\n",
    "    \n",
    "class TestReading(object):\n",
    "    \n",
    "    def test_read(self, setup_data):\n",
    "        path, character = setup_data\n",
    "        # Call the function\n",
    "        read_count(path, character)\n",
    "        # Check the result\n",
    "        with open(f\"character {character}.txt\", 'r') as f:\n",
    "            number = f.read()\n",
    "        assert int(float(number)) == 3, f'value is {int(number)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deja de ser necesario eliminar los archivos luego del yield"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
